{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "In this notebook, we show timings of different parts of **seismiQB**: data generation, segmentation mask creation, model training. We perform each of them in multiple ways, showing different approaches and interfaces provided by our framework (and by **BatchFlow**).\n",
    "\n",
    "Note that this is advanced notebook that requires you to read our other [tutorials and notebooks](./Carcass%20interpolation/01_M_cube.ipynb) to understand what is going on: we don't pay too much time explaining what exactly cells are doing and what is achieved by their code, we merely time it.\n",
    "\n",
    "* [Data loading](data)\n",
    "* [Model architecture](architecture)\n",
    "* [Pipelines: loading, augmentation and training](pipelines)\n",
    "* [Load + augmentation profile](profile1)\n",
    "* [Model training profile](profile2)\n",
    "* [Inference profile](inference)\n",
    "* [Conclusion](conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "sys.path.append('..')\n",
    "from seismiqb.batchflow import Pipeline, FilesIndex\n",
    "from seismiqb.batchflow import B, V, C, F, D, P, R, W\n",
    "from seismiqb.batchflow.batchflow.models.torch import EncoderDecoder, ResBlock # Note the import!\n",
    "\n",
    "from seismiqb import SeismicCubeset, Horizon, plot_image\n",
    "\n",
    "# Set GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "FREQUENCIES = [50]               # carcass frequency at `hard` and `easy` locations\n",
    "CROP_SHAPE = (1, 256, 256)       # shape of sampled 3D crops\n",
    "ITERS = 100                      # number of train iterations\n",
    "BATCH_SIZE = 64                  # number of crops inside one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "# Load everything\n",
    "\n",
    "First of all, we load dataset with seismic cube and a horizon. This operation is performed once per dataset and does not take more that one minute even for all of our cubes (10 total) and horizons (50+ total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cube_path = '/data/seismic/CUBE_2/M_cube.hdf5'\n",
    "horizon_path = '/data/seismic/CUBE_2/RAW/t0_B_anon'\n",
    "\n",
    "dsi = FilesIndex(path=[cube_path], no_ext=True)\n",
    "dataset = SeismicCubeset(dsi)\n",
    "\n",
    "dataset.load_geometries()\n",
    "dataset.create_labels({dataset.indices[0]: [horizon_path]})\n",
    "\n",
    "geometry = dataset.geometries[0]\n",
    "horizon = dataset.labels[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create carcass to train on\n",
    "quality_grid = geometry.make_quality_grid(FREQUENCIES)\n",
    "grid_coverage = (np.nansum(geometry.quality_grid) /\n",
    "                 (np.prod(geometry.cube_shape[:2]) - np.nansum(geometry.zero_traces)))\n",
    "\n",
    "# Create sampler, according to carcass\n",
    "dataset.create_sampler(quality_grid=True)\n",
    "dataset.modify_sampler('train_sampler', finish=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='architecture'></a>\n",
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    # Defining input shapes here allows to build model at initialization\n",
    "    'inputs': {\n",
    "        'images/shape': CROP_SHAPE,\n",
    "        'masks/shape': CROP_SHAPE,\n",
    "    },\n",
    "    \n",
    "    # Model layout\n",
    "    'initial_block': {\n",
    "        'inputs': 'images',\n",
    "        'base_block': ResBlock,\n",
    "        'filters': 16,\n",
    "        'kernel_size': 5,\n",
    "        'downsample': False,\n",
    "    },\n",
    "\n",
    "    'body/encoder': {\n",
    "        'num_stages': 4,\n",
    "        'order': 'sbd',\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'n_reps': 1,\n",
    "            'filters': [16, 32, 64, 128],\n",
    "        },\n",
    "    },\n",
    "    'body/embedding': {\n",
    "        'base': ResBlock,\n",
    "        'n_reps': 1,\n",
    "        'filters': 256,\n",
    "    },\n",
    "    'body/decoder': {\n",
    "        'num_stages': 4,\n",
    "        'upsample': {\n",
    "            'layout': 'bna',\n",
    "            'scale_factor': 2,\n",
    "            'kernel_size': 2,\n",
    "        },\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'filters': [128, 64, 32, 16],\n",
    "        },\n",
    "    },\n",
    "    'head': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': [16, 8],\n",
    "    },\n",
    "    'output': 'sigmoid',\n",
    "    # Train configuration\n",
    "    'loss': 'bdice',\n",
    "    'optimizer': {'name': 'Adam', 'lr': 0.01,},\n",
    "    'decay': {'name': 'exp', 'gamma': 0.1, 'frequency': 150},\n",
    "    'microbatch': 4,\n",
    "    'common/activation': 'relu6',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first call to a GPU takes some time in order to initialize CUDA states; to eliminate this time from actual model initialization time, we manually put some data to the GPU of choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_tensor = np.random.random((10,)).astype(np.float32)\n",
    "\n",
    "tensor = torch.from_numpy(np_tensor)\n",
    "%time tensor = tensor.cuda()\n",
    "\n",
    "tensor = torch.from_numpy(np_tensor)\n",
    "%time tensor = tensor.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we create instance of model directly, without `init_model` action of `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = EncoderDecoder(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipelines'></a>\n",
    "# All the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pipeline = (\n",
    "    Pipeline()\n",
    "\n",
    "    # Load data/masks\n",
    "    .crop(points=D('train_sampler')(BATCH_SIZE),\n",
    "          shape=CROP_SHAPE, adaptive_slices=True)\n",
    "    .create_masks(dst='masks', width=5)\n",
    "    .load_cubes(dst='images')\n",
    "    .adaptive_reshape(src=['images', 'masks'], shape=CROP_SHAPE)\n",
    "    .scale(mode='q', src='images')\n",
    ") << dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pipeline = (\n",
    "    Pipeline()\n",
    "\n",
    "    # Augmentations\n",
    "    .transpose(src=['images', 'masks'], order=(1, 2, 0))\n",
    "    .flip(axis=1, src=['images', 'masks'], seed=P(R('uniform', 0, 1)))\n",
    "    .additive_noise(scale=0.005, src='images', dst='images')\n",
    "    .rotate(angle=P(R('uniform', -15, 15)),\n",
    "            src=['images', 'masks'])\n",
    "    .scale_2d(scale=P(R('uniform', 0.85, 1.15)),\n",
    "              src=['images', 'masks'])\n",
    "    .elastic_transform(alpha=P(R('uniform', 35, 45)),\n",
    "                       sigma=P(R('uniform', 4, 4.5)),\n",
    "                       src=['images', 'masks'])\n",
    "    .transpose(src=['images', 'masks'], order=(2, 0, 1))\n",
    ") << dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = (\n",
    "    Pipeline()\n",
    "\n",
    "    # Initialize pipeline variables and model\n",
    "    .init_variable('loss_history', [])\n",
    "    .import_model(model, name='model')\n",
    "\n",
    "    # Training\n",
    "    .train_model('model',\n",
    "                 fetches='loss',\n",
    "                 images=B('images'),\n",
    "                 masks=B('masks'),\n",
    "                 save_to=V('loss_history', mode='a'))\n",
    ") << dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='profile1'></a>\n",
    "# Data generation and augmentations profile\n",
    "\n",
    "## Regular pipeline usage\n",
    "\n",
    "That is how we usually use pipelines: simple `run` is enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_pipeline = load_pipeline + aug_pipeline\n",
    "data_pipeline.run(D.size, n_iters=ITERS, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the design decisions of our framework is that items in our datasets are cubes: in that notebook, there is only one cube in the `dataset`, therefore, `D.size` evaluates to 1. The logic of converting *batch of cubes* into *batch of crops* is conveniently resides inside `crop` action: under the hood the conversion is performed by creating entirely new batch with generated crop locations. That somewhat confusing behaviour, where one needs to pass *number of cubes* as the `batch size` in `Pipeline.run` method while setting the actual amount of crops elsewhere, allows us to threat tasks with one or multiple cubes the same: there is virtually no changes to do in order to move from the task of carcass interpolation (one cube) to the inter-cube generalization (multiple cubes, as can be deduced by the name).\n",
    "\n",
    "Setting `profile` argument of `run` to `True` allows us to monitor timings of every individual action; it takes some time to parse and log the profiling results, thus giving (a lot of) overhead. Note that the more iteration you run with `profile` on, the slower it becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_pipeline = load_pipeline + aug_pipeline\n",
    "data_pipeline.run(D.size, n_iters=ITERS, bar=True, profile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `show_profile_info` returns a formatted dataframe that can be further explored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_pipeline.show_profile_info()\n",
    "print('Total time of actions running: ', result['total_time']['sum'].sum())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataframe corresponds to individual action: note that we have two `transpose` actions in our augmentation pipeline, thus multiple transposes appear in the table. `total_time` is time take by both action and pipeline inner workings; `pipeline_time` counts only the time of action running. Sub-columns `sum`, `mean` and `max` provide a more detailed description.\n",
    "\n",
    "We can get a more detailed description of which exactly *lines of code* take the most time inside our actions by passing `detailed` argument to the `show_profile_info` method. To avoid cluttering in the notebook, we limit the output to two slowest calls per action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline.show_profile_info(detailed=True, limit=2)['tottime'][['sum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show individual timings for each iteration. That can be helpful to detect memory leaks and other cumulative errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline.show_profile_info(per_iter=True).ix[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [profiling tutorial](https://github.com/analysiscenter/batchflow/blob/master/examples/tutorials/08_profiling.ipynb) to learn more about exact collected information and how to format it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline as data generator\n",
    "\n",
    "`Pipeline` has multiple interfaces; `gen_batch` allows to iterate over batches, using their attributes with data as usual `NumPy` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_pipeline = load_pipeline + aug_pipeline \n",
    "\n",
    "for batch in tqdm(data_pipeline.gen_batch(D.size, n_iters=ITERS), total=ITERS):\n",
    "    images, masks = batch.images, batch.masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass the `profile` argument to the `gen_batch` method. For now, let's just make sure that `images` and `masks` variables contain what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'images: {(type(images), images.dtype, images.shape)}')\n",
    "print(f'masks:  {(type(masks), masks.dtype, masks.shape)}')\n",
    "\n",
    "plot_image((images[0, 0, ...], masks[0, 0, ...]), mode='overlap', y=1.,\n",
    "           xlabel='xlines', ylabel='depth', title='images and masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pipeline to DataLoader\n",
    "\n",
    "Sometimes, `PyTorch DataLoader` is convenient to use. Pipeline is already of a generative nature, so all we need to do is to wrap it into iterable dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineDataset(IterableDataset):\n",
    "    def __init__(self, pipeline, microbatch=4):\n",
    "        self.pipeline = pipeline\n",
    "        self.microbatch = microbatch\n",
    "    \n",
    "    def get_data(self):\n",
    "        while True:\n",
    "            batch = self.pipeline.next_batch(D('size'))\n",
    "            images, masks = batch.images, batch.masks\n",
    "                \n",
    "            for i in range(0, len(images), self.microbatch):\n",
    "                yield images[i:i+self.microbatch, ...], masks[i:i+self.microbatch, ...]\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pds = PipelineDataset(load_pipeline + aug_pipeline, microbatch=64)\n",
    "\n",
    "# Note the `None` batch_size: it is already set as part of the loading pipeline\n",
    "loader =  DataLoader(pds, batch_size=None, pin_memory=True)\n",
    "\n",
    "for batch, _ in tqdm(zip(loader, range(ITERS)), total=ITERS):\n",
    "    images, masks = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'images: {(type(images), images.dtype, images.shape)}')\n",
    "print(f'masks:  {(type(masks), masks.dtype, masks.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader` converts all the data from regular `NumPy` arrays to `Torch.Tensor`s; nevertheless, underlying data is [shared](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='profile2'></a>\n",
    "# Model train profile\n",
    "\n",
    "Most of the cells do exactly the same, yet implore model training step as well.\n",
    "\n",
    "## Regular pipeline usage\n",
    "\n",
    "We run one iteration of something-GPU-related to ensure warm start for all of the subsequent cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_pipeline = load_pipeline + aug_pipeline + model_pipeline\n",
    "train_pipeline.run(D.size, n_iters=1, bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_pipeline.run(D.size, n_iters=ITERS, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can use `profile` flag to get more detailed information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_pipeline = load_pipeline + aug_pipeline + model_pipeline\n",
    "train_pipeline.run(D.size, n_iters=ITERS, bar=True, profile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_pipeline.show_profile_info()\n",
    "print('Total time of actions running: ', result['total_time']['sum'].sum())\n",
    "result['pipeline_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this does not allow to profile individual GPU operations. You can this option in model configuration, in `train` method call or directly into the config of already existing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.full_config['profile'] = True\n",
    "train_pipeline.run(D.size, n_iters=ITERS, bar=True)\n",
    "model.full_config['profile'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it takes a lot of time, though allows us to granularly inspect every cuda kernel time taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_profile_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline as data generator\n",
    "\n",
    "That is roughly the same as what goes under the hood of `model_train` action of `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_pipeline = load_pipeline + aug_pipeline\n",
    "\n",
    "for batch in tqdm(data_pipeline.gen_batch(D.size, n_iters=ITERS), total=ITERS):\n",
    "    images, masks = batch.images, batch.masks\n",
    "    model.train(fetches='loss', images=images, masks=masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pipeline to DataLoader; use Lightning to train the model\n",
    "\n",
    "Obviously, behind our `PyTorch` wrapper lies a plain old `PyTorch` model, that can be accessed via `model` attribute. There are also other attributes to store loss function, optimizer, etc. The `Lightning` wrapper just simply borrows them from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(LightningModule):\n",
    "    def __init__(self, bf_model, pipeline=None):\n",
    "        super().__init__()\n",
    "        self.bf_model = bf_model\n",
    "        self.pipeline = pipeline\n",
    "        \n",
    "        self.loss_list = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bf_model.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.bf_model.train_steps['']['optimizer']\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        predictions = self(images)\n",
    "        \n",
    "        loss_func = self.bf_model.train_steps['']['loss'][0]\n",
    "        loss = loss_func(predictions, targets)\n",
    "        \n",
    "        self.loss_list.append(loss.detach().cpu().numpy())\n",
    "        logs = {'loss': loss}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        if self.pipeline is not None:\n",
    "            pds = PipelineDataset(self.pipeline,\n",
    "                                  microbatch=self.bf_model.config.get('microbatch'))\n",
    "            return DataLoader(pds, batch_size=None, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "l_model = LightningModel(bf_model=model, pipeline=(load_pipeline + aug_pipeline) << dataset)\n",
    "\n",
    "trainer = Trainer(gpus=1,\n",
    "                  accumulate_grad_batches=16,    # reverse microbatch\n",
    "                  max_steps=ITERS,               # number of iterations\n",
    "                  weights_summary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Total iterations number is 800 = 16 * 100 = accumulated batches * ITERS \n",
    "trainer.fit(model=l_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inference'></a>\n",
    "# Inference profile\n",
    "\n",
    "During training we were sampling points along some sparce carcass: that procedure heavily benefits of caching mechanisms in our framework: we need to load very few slices of data during the whole process, so they are instantly loaded into cache and the cube is never touched again.\n",
    "\n",
    "During inference the whole dynamic changes: we are moving from slice to slice sequentially, so each slide is used multiple times (and cache still works wonders), but the overall amount of cube access increases dramatically. Making more overlapping predictions (contolled by the `stride` argument of `make_grid`) benefits more from caching.\n",
    "\n",
    "Action `assemble_crops` is not like the others: it is performed only once, at the end of pipeline run. Specifically:\n",
    "\n",
    "- pipeline iterates over crops, cut from the cube in order to cover the required volume\n",
    "- for each crop, it makes a prediction with trained neural network\n",
    "- after all predictions are available, `assemble_crop` creates a huge 3D array from them, taking overlapping crops into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = (\n",
    "    Pipeline()\n",
    "    # Initialize everything\n",
    "    .init_variable('result_preds', [])\n",
    "    .import_model(model, name='model')\n",
    "\n",
    "    # Load data\n",
    "    .crop(points=D('grid_gen')(),\n",
    "          shape=CROP_SHAPE)\n",
    "    .load_cubes(dst='images')\n",
    "    .adaptive_reshape(src='images', shape=CROP_SHAPE)\n",
    "    .scale(mode='q', src='images')\n",
    "\n",
    "    # Predict with model, then aggregate\n",
    "    .predict_model('model',\n",
    "                   B('images'),\n",
    "                   fetches='predictions',\n",
    "                   save_to=V('result_preds', mode='e'))\n",
    ") << dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset.make_grid(dataset.indices[0], CROP_SHAPE,\n",
    "                  [0, 417], [0, 868], [800, 1000],\n",
    "                  overlap=(1, 96, 96),\n",
    "                  batch_size=BATCH_SIZE*2)\n",
    "\n",
    "inference_pipeline.run(D('size'), n_iters=dataset.grid_iters, bar='n')\n",
    "assembled_pred = dataset.assemble_crops(inference_pipeline.v('result_preds'), order=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no use for the created 3D array: we need an actual 2D surface. That is done via `from_mask` staticmethod of `Horizon` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun -l 10\n",
    "horizons = Horizon.from_mask(assembled_pred, dataset.grid_info,\n",
    "                             minsize=50, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons[-1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion\n",
    "\n",
    "We presented detailed benchmarks for various parts of **seismiQB**, as well as showcased multiple interfaces to do data loading and model training. The timings of each approach are roughly the same, so the real difference is API.\n",
    "\n",
    "This notebook is assumed to be run from time to time to monitor progress and speed-ups with following table to log them:\n",
    "\n",
    "| date, DD.MM.YYYY | load + augmentations, s | load + augmentations + train, s | inference, s |\n",
    "| --- | --- | --- | --- |\n",
    "| 01.06.2019 | ~1600 | ~3600 | ~10000 |\n",
    "| 01.06.2020 | 50 | 115 | 55 |\n",
    "| 26.08.2020 | 45 | 110 | 36 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
